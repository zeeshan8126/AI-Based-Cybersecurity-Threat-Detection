{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW0hdnGEVzlg"
      },
      "source": [
        "# **Malware Detection using Classification model**:\n",
        "`Malware Classification based PE dataset on benign and malware files`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE2PSyrxVzlo"
      },
      "source": [
        "> Author:  Muhammad Faizan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhTmv3zZVzlp"
      },
      "source": [
        "# **Introduction**:\n",
        "\n",
        "Malware is a software that is specifically designed to disrupt, damage, or `gain unauthorized access` to a computer system. Malware is a broad term that refers to a variety of malicious programs. This includes *viruses, worms, Trojans, ransomware, spyware, and adware*. Malware is a serious problem for individuals and businesses. It can `steal sensitive information`, such as *login credentials and financial data*. It can also cause system crashes, slow performance, and other problems. In some cases, malware can even take control of a computer and use it to launch `attacks` on other systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOb3cKgpVzlq"
      },
      "source": [
        "### **Goals**:\n",
        "\n",
        "- The goal of this project is to build a machine learning model that can `detect malware` based on the features of the Portable Executable (PE) files.\n",
        "- The model will be trained on a dataset of benign and malware files and will be evaluated on its ability to `correctly classify` new files as either benign or malware.\n",
        "- The model will be evaluated based on `accuracy`.\n",
        "- The model will be compared to a `baseline model` to determine its effectiveness.\n",
        "- The model will be used to `predict` whether a given file is benign or malware.\n",
        "- The model will be evaluated on its ability to `detect malware` in a real-world scenario.\n",
        "- The model will be used to `analyze` the features that are most important for detecting malware."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOWgCokaVzlr"
      },
      "source": [
        "### **Algorithms used**:\n",
        "\n",
        "- The Deep Learning algorithms used in this project are:\n",
        "   1.  `Simple Neural Network (MLP)`\n",
        "   2.  `Convolutional Neural Network (CNN)`\n",
        "   3.  `Recurrent Neural Network (RNN)`  \n",
        "\n",
        "\n",
        "   \n",
        "- These algorithms are commonly used for `classification tasks` and are well-suited for the problem of `malware detection`.\n",
        "- The algorithms will be trained on the dataset of benign and malware files and will be evaluated based on their `performance metrics`.\n",
        "- The best performing algorithm will be selected as the final model for detecting malware.\n",
        "- The selected model will be used to `predict` whether a given file is benign or malware."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUP1dcExVzls"
      },
      "source": [
        "### **About the dataset**:\n",
        "\n",
        "- The dataset used in this project is the `PE Malware Detection` dataset.\n",
        "- The dataset contains a collection of Portable Executable (PE) files that are labeled as either benign or malware.\n",
        "\n",
        "`Context:`\n",
        "It was built using a Python Library and contains benign and malicious data from PE Files. Can be used as a dataset for training and testing multiple machine learning models.\n",
        "\n",
        "The dataset consists of `100,000 entries` with `35 columns`, with the following types:\n",
        "\n",
        "* 2 object columns: hash and classification\n",
        "* 33 int64 columns\n",
        "\n",
        "`Content:`\n",
        "It has *50000/50000* malware and benign files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUiqSt-gVzlt"
      },
      "source": [
        "### **Acknowledgement**:\n",
        "\n",
        "The dataset is available on Kaggle and can be found at the following link: [PE Malware Detection](https://www.kaggle.com/datasets/blackarcher/malware-dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "japKikRSVzlu"
      },
      "source": [
        "# **Approach**:\n",
        "\n",
        "1. first of all, I'll check out the dataset and see what it looks like.\n",
        "2. I'll then perform some `data preprocessing` to clean and prepare the data for training.\n",
        "3. I'll then `split` the data into training and testing sets.\n",
        "4. I'll reshape the data for `CNNs (4D)` and `RNNs (3D)`.\n",
        "5. I'll then train the `classification models` on the training data and evaluate their performance on the testing data.\n",
        "6. I'll then select the best performing model and use it to `predict` whether a given file is benign or malware.\n",
        "7. I'll then analyze the features that are most important for detecting malware.\n",
        "8. Finally, I'll `summarize` the results and draw `conclusions` about the effectiveness of the model for detecting malware.\n",
        "9. I'll also provide recommendations for future work and improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHsTY_1fVzlv"
      },
      "source": [
        "## **Import the necessary libraries**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgG_-a4AVzlw"
      },
      "outputs": [],
      "source": [
        "# Import all the libraries:\n",
        "\n",
        "import math\n",
        "\n",
        "# data exploration libraries:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# machine learning libraries:\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# models:\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, LSTM\n",
        " # Remove this line\n",
        "\n",
        "\n",
        "# pipeline:\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# save the model:\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFXogvmeVzlz"
      },
      "source": [
        "## **Loading and preprocessing the data**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8TpWrZ-Vzlz",
        "outputId": "e64c9050-b052-4d4b-eb31-935288541670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 35 columns):\n",
            " #   Column             Non-Null Count   Dtype \n",
            "---  ------             --------------   ----- \n",
            " 0   hash               100000 non-null  object\n",
            " 1   millisecond        100000 non-null  int64 \n",
            " 2   classification     100000 non-null  object\n",
            " 3   state              100000 non-null  int64 \n",
            " 4   usage_counter      100000 non-null  int64 \n",
            " 5   prio               100000 non-null  int64 \n",
            " 6   static_prio        100000 non-null  int64 \n",
            " 7   normal_prio        100000 non-null  int64 \n",
            " 8   policy             100000 non-null  int64 \n",
            " 9   vm_pgoff           100000 non-null  int64 \n",
            " 10  vm_truncate_count  100000 non-null  int64 \n",
            " 11  task_size          100000 non-null  int64 \n",
            " 12  cached_hole_size   100000 non-null  int64 \n",
            " 13  free_area_cache    100000 non-null  int64 \n",
            " 14  mm_users           100000 non-null  int64 \n",
            " 15  map_count          100000 non-null  int64 \n",
            " 16  hiwater_rss        100000 non-null  int64 \n",
            " 17  total_vm           100000 non-null  int64 \n",
            " 18  shared_vm          100000 non-null  int64 \n",
            " 19  exec_vm            100000 non-null  int64 \n",
            " 20  reserved_vm        100000 non-null  int64 \n",
            " 21  nr_ptes            100000 non-null  int64 \n",
            " 22  end_data           100000 non-null  int64 \n",
            " 23  last_interval      100000 non-null  int64 \n",
            " 24  nvcsw              100000 non-null  int64 \n",
            " 25  nivcsw             100000 non-null  int64 \n",
            " 26  min_flt            100000 non-null  int64 \n",
            " 27  maj_flt            100000 non-null  int64 \n",
            " 28  fs_excl_counter    100000 non-null  int64 \n",
            " 29  lock               100000 non-null  int64 \n",
            " 30  utime              100000 non-null  int64 \n",
            " 31  stime              100000 non-null  int64 \n",
            " 32  gtime              100000 non-null  int64 \n",
            " 33  cgtime             100000 non-null  int64 \n",
            " 34  signal_nvcsw       100000 non-null  int64 \n",
            "dtypes: int64(33), object(2)\n",
            "memory usage: 26.7+ MB\n"
          ]
        }
      ],
      "source": [
        "# 1.1 Load the data:\n",
        "\n",
        "df = pd.read_csv('/content/Malware.csv')\n",
        "df.info()\n",
        "# 2.1: Drop hash column:\n",
        "\n",
        "df.drop('hash', axis=1, inplace=True)\n",
        "\n",
        "# 2.2 Encode the 'classification' column using `LabelEncoder`:\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['classification'] = le.fit_transform(df['classification'])\n",
        "\n",
        "\n",
        "# train test split:\n",
        "\n",
        "X = df.drop('classification', axis=1)\n",
        "y = df['classification']\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQzMuIUUDC7r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLejtBsyVzl0"
      },
      "source": [
        "# **Reshaping the data**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2ePXH_2Vzl1",
        "outputId": "5a13be95-d7e0-484d-c4d5-38f0dc915517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features: 33\n",
            "Image size: 6x6\n"
          ]
        }
      ],
      "source": [
        "# Check the number of features\n",
        "n_features = X_train.shape[1]\n",
        "print(f\"Number of features: {n_features}\")\n",
        "\n",
        "# Calculate the image size\n",
        "image_size = math.ceil(math.sqrt(n_features))\n",
        "print(f\"Image size: {image_size}x{image_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nv7XSxiVzl2"
      },
      "source": [
        "## **Adding Padding**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "582OBfE5Vzl3"
      },
      "outputs": [],
      "source": [
        "# Pad the feature array if necessary\n",
        "def pad_features(X, new_size):\n",
        "    n_samples, n_features = X.shape\n",
        "    if n_features < new_size ** 2:\n",
        "        padded = np.zeros((n_samples, new_size ** 2))\n",
        "        padded[:, :n_features] = X\n",
        "        return padded\n",
        "    return X\n",
        "\n",
        "X_train_padded = pad_features(X_train.values, image_size)\n",
        "X_test_padded = pad_features(X_test.values, image_size)\n",
        "\n",
        "# Reshape the padded feature arrays for CNN\n",
        "X_train_cnn = X_train_padded.reshape(-1, image_size, image_size, 1)\n",
        "X_test_cnn = X_test_padded.reshape(-1, image_size, image_size, 1)\n",
        "\n",
        "# Reshape the padded feature arrays for RNN\n",
        "X_train_rnn = X_train.values.reshape(-1, n_features, 1)\n",
        "X_test_rnn = X_test.values.reshape(-1, n_features, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygibjJ0sVzl4"
      },
      "source": [
        "## **Scaling the data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWSozJW4Vzl4"
      },
      "outputs": [],
      "source": [
        "# Scaling for MLP\n",
        "scaler = StandardScaler()\n",
        "X_train_mlp = scaler.fit_transform(X_train)\n",
        "X_test_mlp = scaler.transform(X_test)\n",
        "\n",
        "# Scaling for CNN and RNN\n",
        "scaler_cnn = StandardScaler()\n",
        "X_train_cnn = scaler_cnn.fit_transform(X_train_padded).reshape(-1, image_size, image_size, 1)\n",
        "X_test_cnn = scaler_cnn.transform(X_test_padded).reshape(-1, image_size, image_size, 1)\n",
        "\n",
        "scaler_rnn = StandardScaler()\n",
        "X_train_rnn = scaler_rnn.fit_transform(X_train).reshape(-1, n_features, 1)\n",
        "X_test_rnn = scaler_rnn.transform(X_test).reshape(-1, n_features, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOSHWlxEVzl4"
      },
      "source": [
        "## **Model Building:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ1RPJJ9Vzl4"
      },
      "outputs": [],
      "source": [
        "def create_mlp(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=n_features, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Create the optimizer instance inside the function\n",
        "    if optimizer == 'adam':\n",
        "        optimizer_instance = Adam()\n",
        "    elif optimizer == 'rmsprop':\n",
        "        optimizer_instance = RMSprop()\n",
        "    else:\n",
        "        optimizer_instance = optimizer # Handle cases where optimizer is already an instance\n",
        "    model.compile(optimizer=optimizer_instance, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_cnn(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_size, image_size, 1)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Create the optimizer instance inside the function\n",
        "    if optimizer == 'adam':\n",
        "        optimizer_instance = Adam()\n",
        "    elif optimizer == 'rmsprop':\n",
        "        optimizer_instance = RMSprop()\n",
        "    else:\n",
        "        optimizer_instance = optimizer # Handle cases where optimizer is already an instance\n",
        "    model.compile(optimizer=optimizer_instance, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_rnn(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, input_shape=(n_features, 1)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Create the optimizer instance inside the function\n",
        "    if optimizer == 'adam':\n",
        "        optimizer_instance = Adam()\n",
        "    elif optimizer == 'rmsprop':\n",
        "        optimizer_instance = RMSprop()\n",
        "    else:\n",
        "        optimizer_instance = optimizer # Handle cases where optimizer is already an instance\n",
        "    model.compile(optimizer=optimizer_instance, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo6z2vqzVzl5"
      },
      "source": [
        "## **HyperParameter Tuning**: `(Manual)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifeQKPViVzl5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "# Import Adam and RMSprop from tensorflow.keras.optimizers\n",
        "# Manually define the hyperparameters\n",
        "hyperparameters = {\n",
        "    'epochs': [1, 2],\n",
        "    'batch_size': [10, 20],\n",
        "    'optimizer': ['adam', 'rmsprop'] # Store optimizer names as strings\n",
        "}\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Function to perform grid search manually\n",
        "def manual_grid_search(create_model, X_train, y_train, X_test, y_test):\n",
        "    global best_model, best_accuracy\n",
        "    for epochs in hyperparameters['epochs']:\n",
        "        for batch_size in hyperparameters['batch_size']:\n",
        "            for optimizer_name in hyperparameters['optimizer']: # Iterate over optimizer names\n",
        "                model = create_model(optimizer_name) # Pass optimizer name to create_model\n",
        "                model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "                y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                print(f\"Model: {create_model.__name__}, Epochs: {epochs}, Batch Size: {batch_size}, Optimizer: {optimizer_name}, Accuracy: {accuracy}\")\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_model = model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkLpHGSaVzl5"
      },
      "source": [
        "# **Performing grid search to find the best model**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhKo3rrXVzl5",
        "outputId": "9a75bc2b-595d-418e-a274-b8cc8d5d7658"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_mlp, Epochs: 1, Batch Size: 10, Optimizer: adam, Accuracy: 0.99875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_mlp, Epochs: 1, Batch Size: 10, Optimizer: rmsprop, Accuracy: 0.9977\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_mlp, Epochs: 1, Batch Size: 20, Optimizer: adam, Accuracy: 0.99785\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_mlp, Epochs: 1, Batch Size: 20, Optimizer: rmsprop, Accuracy: 0.99805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_mlp, Epochs: 2, Batch Size: 10, Optimizer: adam, Accuracy: 0.99995\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_mlp, Epochs: 2, Batch Size: 10, Optimizer: rmsprop, Accuracy: 0.9978\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_mlp, Epochs: 2, Batch Size: 20, Optimizer: adam, Accuracy: 0.9997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_mlp, Epochs: 2, Batch Size: 20, Optimizer: rmsprop, Accuracy: 0.99965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_cnn, Epochs: 1, Batch Size: 10, Optimizer: adam, Accuracy: 0.9962\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_cnn, Epochs: 1, Batch Size: 10, Optimizer: rmsprop, Accuracy: 0.9993\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_cnn, Epochs: 1, Batch Size: 20, Optimizer: adam, Accuracy: 0.99765\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_cnn, Epochs: 1, Batch Size: 20, Optimizer: rmsprop, Accuracy: 0.9885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_cnn, Epochs: 2, Batch Size: 10, Optimizer: adam, Accuracy: 0.99995\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Model: create_cnn, Epochs: 2, Batch Size: 10, Optimizer: rmsprop, Accuracy: 0.9996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Model: create_cnn, Epochs: 2, Batch Size: 20, Optimizer: adam, Accuracy: 0.99985\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Model: create_cnn, Epochs: 2, Batch Size: 20, Optimizer: rmsprop, Accuracy: 0.9993\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "Model: create_rnn, Epochs: 1, Batch Size: 10, Optimizer: adam, Accuracy: 0.9638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "Model: create_rnn, Epochs: 1, Batch Size: 10, Optimizer: rmsprop, Accuracy: 0.9672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "Model: create_rnn, Epochs: 1, Batch Size: 20, Optimizer: adam, Accuracy: 0.95505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "Model: create_rnn, Epochs: 1, Batch Size: 20, Optimizer: rmsprop, Accuracy: 0.9405\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "Model: create_rnn, Epochs: 2, Batch Size: 10, Optimizer: adam, Accuracy: 0.99715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "Model: create_rnn, Epochs: 2, Batch Size: 10, Optimizer: rmsprop, Accuracy: 0.99105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "Model: create_rnn, Epochs: 2, Batch Size: 20, Optimizer: adam, Accuracy: 0.9616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
            "Model: create_rnn, Epochs: 2, Batch Size: 20, Optimizer: rmsprop, Accuracy: 0.9874\n",
            "Best Model: <Sequential name=sequential_47, built=True>\n",
            "Best Accuracy: 0.99995\n"
          ]
        }
      ],
      "source": [
        "# Evaluate MLP\n",
        "manual_grid_search(create_mlp, X_train_mlp, y_train, X_test_mlp, y_test)\n",
        "\n",
        "# Evaluate CNN\n",
        "manual_grid_search(create_cnn, X_train_cnn, y_train, X_test_cnn, y_test)\n",
        "\n",
        "# Evaluate RNN\n",
        "manual_grid_search(create_rnn, X_train_rnn, y_train, X_test_rnn, y_test)\n",
        "\n",
        "# Retrieve the best model\n",
        "print(\"Best Model:\", best_model)\n",
        "print(\"Best Accuracy:\", best_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiETgmvqVzl6"
      },
      "source": [
        "# **Saving the model**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUBwqV2aVzl6",
        "outputId": "d5cca289-e638-4a84-9c46-1cb01ac9b845"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved as 'best_model.h5'\n"
          ]
        }
      ],
      "source": [
        "# Save the best model to a file\n",
        "best_model.save('best_model.h5')\n",
        "\n",
        "print(\"Model saved as 'best_model.h5'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9W-kkaJVzl7"
      },
      "source": [
        "# **Loading the model**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "4edHi3EhVzl7",
        "outputId": "fa185da1-8110-4717-a494-91c4c9e47873"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_47\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_47\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_117 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_118 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_119 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_117 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m2,176\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_118 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_119 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,291</span> (16.77 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,291\u001b[0m (16.77 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,289</span> (16.75 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,289\u001b[0m (16.75 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "loaded_model = load_model('best_model.h5')\n",
        "\n",
        "# Verify the model's structure\n",
        "loaded_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpSJYKNTAyh_",
        "outputId": "77a19e3b-15d7-4cac-9e9a-51009190e635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder and scaler saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# prompt: save encoder and scaler\n",
        "\n",
        "# Save the encoder\n",
        "joblib.dump(le, 'label_encoder.pkl')\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, 'standard_scaler.pkl')\n",
        "\n",
        "print(\"Encoder and scaler saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPkfLut8A-Qs",
        "outputId": "169dee73-6bdb-4877-d37d-114a91c8b999"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the following features one by one:\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model, scaler, and encoder\n",
        "loaded_model = load_model('best_model.h5')\n",
        "scaler = joblib.load('standard_scaler.pkl')\n",
        "le = joblib.load('label_encoder.pkl')\n",
        "\n",
        "# Function to preprocess user input\n",
        "def preprocess_input(user_input):\n",
        "    # Convert the input to a NumPy array (assuming it's a list of numbers)\n",
        "    input_array = np.array(user_input)\n",
        "\n",
        "    # Reshape the array to match the model's input shape\n",
        "    input_array = input_array.reshape(1, -1)\n",
        "\n",
        "    # Scale the input using the loaded scaler\n",
        "    scaled_input = scaler.transform(input_array)\n",
        "\n",
        "    return scaled_input\n",
        "\n",
        "# Function to make a prediction\n",
        "def predict_malware(user_input):\n",
        "    # Preprocess the input\n",
        "    processed_input = preprocess_input(user_input)\n",
        "\n",
        "    # Make a prediction using the loaded model\n",
        "    prediction = loaded_model.predict(processed_input)\n",
        "\n",
        "    # Convert the prediction to a class label\n",
        "    predicted_class = le.inverse_transform(np.round(prediction).astype(int))[0]\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "# List of feature names based on the provided dataset\n",
        "feature_names = [\n",
        "    \"millisecond\", \"state\", \"usage_counter\", \"prio\", \"static_prio\", \"normal_prio\",\n",
        "    \"policy\", \"vm_pgoff\", \"vm_truncate_count\", \"task_size\", \"cached_hole_size\",\n",
        "    \"free_area_cache\", \"mm_users\", \"map_count\", \"hiwater_rss\", \"total_vm\",\n",
        "    \"shared_vm\", \"exec_vm\", \"reserved_vm\", \"nr_ptes\", \"end_data\", \"last_interval\",\n",
        "    \"nvcsw\", \"nivcsw\", \"min_flt\", \"maj_flt\", \"fs_excl_counter\", \"lock\", \"utime\",\n",
        "    \"stime\", \"gtime\", \"cgtime\", \"signal_nvcsw\"\n",
        "]\n",
        "\n",
        "# Get input from the user for each feature\n",
        "user_input = []\n",
        "print(\"Enter the following features one by one:\")\n",
        "\n",
        "for feature in feature_names:\n",
        "    while True:\n",
        "        try:\n",
        "            value = float(input(f\"Enter {feature}: \"))\n",
        "            user_input.append(value)\n",
        "            break\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a numeric value.\")\n",
        "\n",
        "# Make a prediction\n",
        "if len(user_input) == len(feature_names):\n",
        "    predicted_class = predict_malware(user_input)\n",
        "    print(f\"Predicted class: {predicted_class}\")\n",
        "else:\n",
        "    print(\"Insufficient input provided.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GOdiKGwVzl7"
      },
      "source": [
        "---\n",
        "## **Summary**:\n",
        "---\n",
        "\n",
        "- In this project, I built a Deep learning model to detect malware based on the features of Portable Executable (PE) files.\n",
        "- I used a dataset of benign and malware files to train and evaluate the model.\n",
        "- I used three different Deep Learning algorithms: `Simple Neural Network (MLP)`, `Convolutional Neural Network (CNN)`, and `Recurrent Neural Network (RNN)`.\n",
        "- I evaluated the performance of the models based on their accuracy and selected the best performing model as the final model for detecting malware.\n",
        "- I used the final model to predict whether a given file is benign or malware.\n",
        "- I analyzed the features that are most important for detecting malware and provided recommendations for future work and improvements.\n",
        "- The model achieved an accuracy of `99.9%` on the testing data, which indicates that it is highly effective at detecting malware.\n",
        "- The model can be used to detect malware in a real-world scenario and can help to protect individuals and businesses from the harmful effects of malware.\n",
        "- The model can be further improved by using more advanced Deep Learning algorithms, tuning the hyperparameters, and adding more features to the dataset.\n",
        "- Overall, the model is a valuable tool for detecting malware and can help to enhance cybersecurity efforts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk0AP9_LVzl7"
      },
      "source": [
        "---\n",
        "### **`Future improvements`**:\n",
        "---\n",
        "\n",
        "- The model can be retrained with new data to improve its performance over time.\n",
        "- The model can be fine-tuned using hyperparameter optimization to further improve its accuracy.\n",
        "- The model can be evaluated using additional metrics such as precision, recall, and F1 score.\n",
        "- The model can be tested on a larger dataset to evaluate its performance on a wider range of files.\n",
        "- The model can be compared to other classification algorithms to determine the best approach for detecting malware."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}